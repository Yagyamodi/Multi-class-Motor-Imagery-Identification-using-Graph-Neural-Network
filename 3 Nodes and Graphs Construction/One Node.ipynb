{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"One Node.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUPsW1kW43Qm","executionInfo":{"status":"ok","timestamp":1636525729579,"user_tz":-330,"elapsed":31912,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}},"outputId":"42e6cfce-92d8-4d95-a264-5d5dec5abf8b"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7szQc6o5NyP","executionInfo":{"status":"ok","timestamp":1636525732963,"user_tz":-330,"elapsed":1725,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}},"outputId":"e1e830a3-1db3-468c-fa3b-18d2bbc8effd"},"source":["cd \"/content/drive/MyDrive/BTP_AMRITAMAM/\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1HAUgj9ICrfB46ME_Gce9FK26De_hM_Yv/BTP_AMRITAMAM\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqL6Cyfc5RAF","executionInfo":{"status":"ok","timestamp":1636525759559,"user_tz":-330,"elapsed":24865,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}},"outputId":"ed3c29dc-95ad-4c6a-ab63-67923b1747bf"},"source":["#!pip install spectrum\n","#!pip install git+https://github.com/synergetics/spectrum.git\n","!pip install hfda\n","#!pip install -q entropy==v0.1.0 thi is deprecated and the new one is below\n","!pip install antropy\n","!pip install git+https://github.com/forrestbao/pyeeg.git # ap, sample, svd entropies---> pyeeg\n","#!pip install eeglib \n","!pip install ordpy #permutation entropy feature\n","!pip install turbustat"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hfda\n","  Downloading hfda-0.1.1.tar.gz (1.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hfda) (1.19.5)\n","Building wheels for collected packages: hfda\n","  Building wheel for hfda (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hfda: filename=hfda-0.1.1-py3-none-any.whl size=2159 sha256=cb2271b8a49e1d55d34b4c82ee2878889e2d9f2a17b64d6acec8f943d0dacb0e\n","  Stored in directory: /root/.cache/pip/wheels/cc/cf/2e/ee8b2442bdfd74fb48c59230ac1ab1d32574e8d41130ded745\n","Successfully built hfda\n","Installing collected packages: hfda\n","Successfully installed hfda-0.1.1\n","Collecting antropy\n","  Downloading antropy-0.1.4.tar.gz (16 kB)\n","Collecting stochastic\n","  Downloading stochastic-0.6.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 3.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from stochastic->antropy) (1.19.5)\n","Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from stochastic->antropy) (1.4.1)\n","Building wheels for collected packages: antropy\n","  Building wheel for antropy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antropy: filename=antropy-0.1.4-py3-none-any.whl size=16158 sha256=e19f5892dbf652a297a83bd80d6c818b0a61e0073be9e17afdc026555e34f254\n","  Stored in directory: /root/.cache/pip/wheels/d4/cc/6b/28fa7036bdabc6f1f92fd6a108ae259dbb48f107f676d6b491\n","Successfully built antropy\n","Installing collected packages: stochastic, antropy\n","Successfully installed antropy-0.1.4 stochastic-0.6.0\n","Collecting git+https://github.com/forrestbao/pyeeg.git\n","  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-88w53xsn\n","  Running command git clone -q https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-88w53xsn\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyeeg==0.4.4) (1.19.5)\n","Building wheels for collected packages: pyeeg\n","  Building wheel for pyeeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyeeg: filename=pyeeg-0.4.4-py2.py3-none-any.whl size=28132 sha256=3f9d14813b3b9a65ec76a644565e75298e3c63f808c17982e00e3ebff2c7a4f7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-iur6as3x/wheels/b0/23/e4/703c908bda656959957029fa145879aa79307b2545a2ef0271\n","Successfully built pyeeg\n","Installing collected packages: pyeeg\n","Successfully installed pyeeg-0.4.4\n","Collecting ordpy\n","  Downloading ordpy-1.0.6-py3-none-any.whl (20 kB)\n","Installing collected packages: ordpy\n","Successfully installed ordpy-1.0.6\n","Collecting turbustat\n","  Downloading turbustat-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.13 in /usr/local/lib/python3.7/dist-packages (from turbustat) (0.22.2.post1)\n","Requirement already satisfied: matplotlib>=1.2 in /usr/local/lib/python3.7/dist-packages (from turbustat) (3.2.2)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from turbustat) (0.16.2)\n","Requirement already satisfied: statsmodels>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from turbustat) (0.10.2)\n","Collecting spectral-cube\n","  Downloading spectral_cube-0.6.0-py3-none-any.whl (218 kB)\n","\u001b[K     |████████████████████████████████| 218 kB 51.5 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.21 in /usr/local/lib/python3.7/dist-packages (from turbustat) (1.4.1)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from turbustat) (1.19.5)\n","Requirement already satisfied: astropy>=2.0 in /usr/local/lib/python3.7/dist-packages (from turbustat) (4.3.1)\n","Requirement already satisfied: pyerfa>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from astropy>=2.0->turbustat) (2.0.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from astropy>=2.0->turbustat) (4.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.2->turbustat) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.2->turbustat) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.2->turbustat) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.2->turbustat) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.2->turbustat) (1.15.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->turbustat) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->turbustat) (2.6.3)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->turbustat) (7.1.2)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->turbustat) (1.1.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.13->turbustat) (1.1.0)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.4.0->turbustat) (0.5.2)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.4.0->turbustat) (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels>=0.4.0->turbustat) (2018.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->astropy>=2.0->turbustat) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->astropy>=2.0->turbustat) (3.10.0.2)\n","Collecting casa-formats-io\n","  Downloading casa_formats_io-0.1-cp37-cp37m-manylinux2010_x86_64.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: dask[array] in /usr/local/lib/python3.7/dist-packages (from spectral-cube->turbustat) (2.12.0)\n","Collecting radio-beam>=0.3.3\n","  Downloading radio_beam-0.3.3-py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[array]->spectral-cube->turbustat) (0.11.1)\n","Installing collected packages: radio-beam, casa-formats-io, spectral-cube, turbustat\n","Successfully installed casa-formats-io-0.1 radio-beam-0.3.3 spectral-cube-0.6.0 turbustat-1.2.1\n"]}]},{"cell_type":"code","metadata":{"id":"GtwtVYis8bRt","executionInfo":{"status":"ok","timestamp":1636525765983,"user_tz":-330,"elapsed":6428,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}}},"source":["import csv\n","import numpy as np\n","np.warnings.filterwarnings('ignore')\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import plotly.graph_objects as go\n","import os  \n","import hfda\n","import math\n","import statistics\n","from scipy.stats import kurtosis\n","#import entropy as ent\n","#import eeglib\n","import scipy as sp\n","import antropy\n","import pyeeg\n","import ordpy\n","import networkx as nx\n","#import spectrum"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Rv6V3kO5X8H","executionInfo":{"status":"ok","timestamp":1636525768102,"user_tz":-330,"elapsed":722,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}}},"source":["#------------------STATISCTICAL FEATURES-------------------\n","\n","def peakValue(X):\n","    val = np.amax(abs(np.array(X)))\n","\n","    return val\n","\n","def rmsValue(X):\n","    n = X.shape[0]\n","\n","    square = np.sum(np.array(X)**2)\n","    mean = (square / (float)(n))\n","    root = math.sqrt(mean)\n","     \n","    root = root/4       # normalisation\n","    #print(\"root mean square value: \", root)\n","    return root\n","\n","def absoluteMean(X):\n","    mean = np.sum(abs(np.array(X)))\n","    \n","    mean = mean/3000    #normalisation\n","    #print(\"Absolute mean: \", float(mean))\n","    return mean\n","\n","def totalVariation(X):    \n","    val = np.sum(abs(np.diff(X)))\n","\n","    val = val/1500    #normalisation\n","    #print(\"total variation: \", float(val))\n","    return val\n","\n","def hurst(X):\n","    X = np.array(X)\n","    N = X.size\n","    T = np.arange(1, N + 1)\n","    Y = np.cumsum(X)\n","    Ave_T = Y / T\n","\n","    S_T = np.zeros(N)\n","    R_T = np.zeros(N)\n","\n","    for i in range(N):\n","        S_T[i] = np.std(X[:i + 1])\n","        X_T = Y - T * Ave_T[i]\n","        R_T[i] = np.ptp(X_T[:i + 1])\n","\n","    R_S = R_T / S_T\n","    R_S = np.log(R_S)[1:]\n","    n = np.log(T)[1:]\n","    A = np.column_stack((n, np.ones(n.size)))\n","    [m, c] = np.linalg.lstsq(A, R_S, rcond = None)[0]\n","    H = m\n","    return H\n","\n","\n","def dfa(X, Ave=None, L=None):     # pyeeg official implementation\n","    X = pd.DataFrame(X).to_numpy()\n","    X = X.flatten()\n","    if Ave is None:\n","        Ave = np.mean(X)\n","\n","    Y = np.cumsum(X)\n","    Y -= Ave\n","\n","    if L is None:\n","        L = np.floor(len(X) * 1 / (\n","            2 ** np.array(list(range(4, int(np.log2(len(X))) - 4))))\n","        )\n","\n","    F = np.zeros(len(L))  # F(n) of different given box length n\n","\n","    for i in range(0, len(L)):\n","        n = int(L[i])                        # for each box length L[i]\n","        for j in range(0, len(X), n):  # for each box\n","            if j + n < len(X):\n","                c = list(range(j, j + n))\n","                # coordinates of time in the box\n","                c = np.vstack([c, np.ones(n)]).T\n","                # the value of data in the box\n","                y = Y[j:j + n]\n","                # add residue in this box\n","                F[i] += np.linalg.lstsq(c, y)[1]\n","        F[i] /= ((len(X) / n) * n)\n","    F = np.sqrt(F)\n","\n","    Alpha = np.linalg.lstsq(np.vstack(\n","        [np.log(L), np.ones(len(L))]\n","    ).T, np.log(F))[0][0]\n","\n","    #print(\"Detrended Fluctuation Analysis (dfa): \", Alpha)\n","    return Alpha\n","\n","\n","def hjorth(X, D=None):    # pyeeg official implementation\n","    X = pd.DataFrame(X).to_numpy()\n","    X = X.flatten()\n","    if D is None:\n","        D = np.diff(X)\n","        D = D.tolist()\n","\n","    D.insert(0, X[0])  # pad the first difference\n","    D = np.array(D)\n","\n","    n = len(X)\n","\n","    M2 = float(sum(D ** 2)) / n\n","    TP = sum(np.array(X) ** 2)\n","    M4 = np.sum(np.diff(D)**2)\n","    M4 = M4 / n\n","\n","    complexity = np.sqrt(float(M4) * TP / M2 / M2)\n","    complexity = (complexity - 35) / 10          # normalisation\n","    mobility = np.sqrt(M2 / TP)\n","    mobility = mobility*10    # normalisation\n","    return mobility, complexity  # Hjorth Mobility and Complexity\n","\n","\n","# ---------------FRACTAL FEATURES---------------------------\n","\n","def HiguchiFractal(X, k_max):\n","    D = hfda.measure(X, k_max)\n","    return D-1    # returning (D-1) instead of returning real answer, i.e., D because for normalisation I want all values in range(0,1)\n","\n","def KatzFractal(X):\n","    d = -100000\n","    n = X.shape[0]\n","    L = 0.000000\n","    for i in range(0,n-2):\n","      tmp1 = (X.loc[i+1]-X.loc[i])*(X.loc[i+1]-X.loc[i]) + 0.4*0.4;\n","      tmp1 = math.sqrt(tmp1)\n","      L += tmp1\n","      tmp2 = math.sqrt((X.loc[i+1]-X.loc[0])*(X.loc[i+1]-X.loc[0]) + (0.4*i)*(0.4*i))    \n","      d = max(d,tmp2)\n","    \n","    a = math.log(n)\n","    D = a/(a+math.log(d/L))\n","\n","    #print(\"Katz Fractal \", D-1);\n","    return D-1\n","\n","def PetrosianFractal(X):\n","    mean = 0.000000\n","    n = X.shape[0]\n","    mean = np.sum(np.array(X))\n","\n","    mean = mean/n\n","\n","    z = [[0] * 1] * n   # n rows, 1 column, all equal to zero\n","    #func = lambda x : 1 if (x > mean) else -1\n","    #z = func(np.array(X))\n","    for i in range(0,n):\n","        if (X.loc[i] > mean).bool():\n","          z[i] = 1\n","        else:\n","          z[i] = -1\n","\n","    nDel = np.sum(abs(np.diff(z)))\n","\n","    nDel = (1.00*nDel)/2\n","    a = math.log10(n)\n","    D = a/(a + math.log10(n/(n+0.4*nDel)))\n","\n","    #print(\"Petrosian Fractal \", D-1)\n","    return D-1\n","\n","def SevcikFractal(X):\n","    n = X.shape[0]\n","    x_min = 100000000\n","    y_min = 0\n","    x_max = -10000000000\n","    y_max = 0.4*(n-1)\n","\n","    xStar = [0]*n\n","    yStar = [0]*n\n","\n","    X1 = np.array(X)\n","    x_max = np.amax(X1)\n","    x_min = np.amin(X1)\n","\n","    xStar = (X1 - x_min) / (x_max - x_min)\n","    yStar = np.linspace(0, (0.4*(n-1) -y_min)/(y_max-y_min), n)\n","    xStar = (np.diff(xStar, axis = 0)**2).reshape(xStar.size-1,)\n","    yStar = np.diff(yStar)**2\n","\n","    L = np.sum(np.sqrt(xStar + yStar))\n","\n","    D = 1 + (math.log(L) + math.log(2))/math.log(2*(n-1))\n","    \n","    #print(\"Sevcik Fractal \", D-1)\n","    return D-1\n","\n","def findSquareMean(X):\n","    mean = np.sum(np.array(X)**2)\n","\n","    #print(\"mean \", mean)\n","    return mean\n","\n","def HajorthParameters(xV):\n","    n = xV.shape[0];\n","    dxV = np.diff([xV]);\n","    ddxV = np.diff([dxV]);\n","    mx2 = findSquareMean(xV);\n","    mdx2 = findSquareMean(dxV);\n","    mddx2 = findSquareMean(ddxV);\n","\n","    mob = mdx2 / mx2;\n","    complexity = math.sqrt(mddx2 / mdx2 - mob);\n","    mobility = math.sqrt(mob);\n","\n","    #print(\"Hajorth parameters: mobility = \", mobility, \", complexity = \", complexity);\n","    return {mobility, complexity};\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrHTPxnM9CqA","executionInfo":{"status":"ok","timestamp":1636525776245,"user_tz":-330,"elapsed":431,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}}},"source":["# Make All Nodes of one csv file\n","\n","def makeGraphNodes(readFile):\n","    training = pd.read_csv(readFile)\n","    #makeFolder(s,saveDirectory)\n","    n = training.shape[1]\n","    all_nodes = []\n","\n","    for node_no in range (0,9):\n","        labelling = [0] * 17\n","\n","        i = 0\n","        # Get Kth Column of Matrix\n","        x = training.iloc [ : , node_no :node_no+1 ]\n","        X=np.array(training).T[node_no]\n","        #print(\"data \", x)\n","\n","        # Statistical features\n","        labelling[0] = kurtosis(x, fisher = True)[0]  #fisher: If True, Fisher’s definition is used (normal ==> 0.0). If False, Pearson’s definition is used (normal ==> 3.0).\n","        #print(\"Kurtosis: \", labelling[0])\n","        #labelling[1] = peakValue(x)\n","        labelling[1] = rmsValue(x)\n","        #labelling[3] = (labelling[1]/labelling[2])**2   # Peak-to-Average Power Ratio\n","        #labelling[4] = (labelling[2]/absoluteMean(x))   # Form factor\n","        labelling[2] = totalVariation(x)\n","        labelling[3] = hurst(x)\n","        #print(\"Hurst exponent: \", labelling[3])\n","        labelling[4] = dfa(x)\n","        labelling[5], labelling[6] = hjorth(x)\n","        #print(\"Hjorth mobility, complexity: \", labelling[8], \" \", labelling[9])\n","        labelling[7] = pyeeg.fisher_info(X, 5, 2)\n","        #print(\"fisher info\", labelling[10]) \n","\n","        # Fractal features\n","        labelling[8] = antropy.higuchi_fd(X);\n","        labelling[8] = labelling[8] - 1     # normalisation\n","        #print(\"higuchi fractal dimension\", labelling[11])\n","        labelling[9] = KatzFractal(x)\n","        labelling[10] = PetrosianFractal(x)\n","        labelling[11] = SevcikFractal(x)\n","\n","        # Entropy features\n","        s=np.std(X)\n","        labelling[12] = pyeeg.ap_entropy(X, 2, 0.2*s)\n","        #Sample Entropy : s(X,m,r) : Generally we take the value of m to be 2 and the value of r to be 0.2*std.\n","        #the above also applies for ap_entropy  \n","        #print(\"\\nap entropy: \", labelling[15])\n","        \n","        labelling[13] = antropy.sample_entropy(X) \n","        #print(\"simple entropy: \", labelling[16])\n","\n","        labelling[14] = antropy.svd_entropy(X, 5, 2, normalize=True)\n","        #print(\"svd (using entropy library) entropy: \", labelling[17])\n","        \n","        labelling[15] = antropy.perm_entropy(X, 5, 2, normalize=True)\n","        #print(\"permutation entropy: \", labelling[18])\n","        \n","        labelling[16] = antropy.spectral_entropy(X, sf=250, method='welch', normalize=True) # sf is sampling frequency\n","        #print(\"spectral entropy: \", labelling[19])\n","\n","        all_nodes.append(labelling)\n","       \n","    \n","    return all_nodes\n","    "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdSjpvUU2t8s","executionInfo":{"status":"ok","timestamp":1636525779666,"user_tz":-330,"elapsed":471,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}}},"source":["# Adjacency Matrix representation in Python\n","\n","\n","class Graph(object):\n","\n","    # Initialize the matrix\n","    def __init__(self, size):\n","        self.adjMatrix = []\n","        for i in range(size):\n","            self.adjMatrix.append([0 for i in range(size)])\n","        self.size = size\n","\n","    # Add edges\n","    def add_edge(self, v1, v2, weight):\n","        self.adjMatrix[v1][v2] = weight\n","        self.adjMatrix[v2][v1] = weight\n","\n","    # Remove edges\n","    def remove_edge(self, v1, v2):\n","        if self.adjMatrix[v1][v2] == 0:\n","            print(\"No edge between %d and %d\" % (v1, v2))\n","            return\n","        self.adjMatrix[v1][v2] = 0\n","        self.adjMatrix[v2][v1] = 0\n","\n","    def __len__(self):\n","        return self.size\n","\n","    # Print the matrix\n","    def print_matrix(self):\n","        for row in self.adjMatrix:\n","            for val in row:\n","                print('{:4}'.format(val), end = \" \"),\n","            print(\"\")\n","\n","def findWeight(v1, v2):\n","    # find gaussian F(a,b)\n","    sigma = 100.000\n","\n","    d = np.sum((np.float_(v1) - np.float_(v2))**2)\n","    d = math.sqrt(d)\n","    gaussian = math.exp((-1.0000*d*d)/(2*sigma*sigma))\n","    #print(\"gaussian \", round(gaussian,8), end = \", \")\n","\n","    v1 = np.array(v1)\n","    v2 = np.array(v2)\n","    Ta = np.sum(v1**2.00)\n","    Tb = np.sum(v2**2.00)\n","    Ta = math.sqrt(Ta)\n","    Tb = math.sqrt(Tb)\n","    d2 = math.log(Ta/Tb)\n","    #print(\"d \", round(d2,8), end=\" , \")\n","\n","    weight = round(gaussian,8)*round(d2,8)\n","    if(weight<0.00):\n","      weight = -1.00*weight\n","\n","    return round(weight,5)\n","\n","\n","def mainFunc(all_nodes):\n","    l = len(all_nodes)\n","    g = Graph(l)\n","    for i in range(0,l):\n","        for j in range(i,l):\n","            weight = findWeight(all_nodes[i],all_nodes[j])\n","            g.add_edge(i,j,weight)\n","\n","    \n","    A = g.adjMatrix\n","    #G = nx.from_numpy_matrix(np.matrix(A), create_using=nx.DiGraph)\n","    \n","  \n","    #posi = nx.circular_layout(G)\n","    #color=['green', 'yellow', 'red', 'blue', 'purple', 'brown', 'cyan', 'grey', 'black'] #'gray', 'olive', 'orange', 'pink', 'gold', 'lightsteelblue', 'peru', 'peachpuff', 'darkseagreen', 'darkkhaki', 'black', 'snow', 'wheat']\n","    # larger figure size\n","    #plt.figure(5,figsize=(7,7))\n","    ###nx.draw(G, posi)\n","    #nx.draw(G, posi, node_color=color)\n","    #nx.draw_networkx_edge_labels(G, pos=posi)\n","    \n","    #plt.show()\n","\n","    #g.print_matrix()\n","    return A\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"hrEhnhphBEzc","executionInfo":{"elapsed":808,"status":"ok","timestamp":1633785845327,"user":{"displayName":"Kartik Rai 5-Yr IDD Comp. Sci. & Engg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15050916069074999926"},"user_tz":-330},"outputId":"46fdc4c2-26bb-46ec-91e7-248466c18dbc"},"source":["#code for making adjacency matrix\n","'''\n","import csv\n","import scipy.signal\n","import numpy as np\n","import pandas as pd\n","from skimage import io\n","import os  \n","    \n","def makeFolder(directory,parent_dir):\n","      # os.makedirs() method will raise  \n","      # an OSError if the directory to be created already exists  \n","          \n","      # Path  \n","      path = os.path.join(parent_dir, directory)  \n","          \n","      # Create the directory  \n","      os.makedirs(path)  \n","      print(\"Directory '% s' created\" % directory) \n","      return 1\n","\n","\n","#for all A01-9 folder\n","for folder in range(8,10):\n","    #for all A01T_1-4 folder\n","    for file in range(3,5):\n","        s = \"A0\" + str(folder) + \"E_\" + str(file);   #read file name\n","        readFolder = \"Segmentation_after_channel_selection/A0\"+ str(folder)+\"/Testing/\" + s + \"/\"   #read file location\n","        writeFolder = \"Graphs/A0\"+str(folder)+\"/Testing/\"  #save location\n","        #makeFolder(s,writeFolder)\n","        writeFolder += s + \"/\"\n","        dirListing = os.listdir(readFolder)\n","        print(len(dirListing))\n","        file_count = len(dirListing)\n","        for subfileNo in range(1,95):\n","            readFile = readFolder + str(subfileNo) + \".csv\"\n","            print(readFile)\n","            #training = pd.read_csv(readFile)\n","            all_nodes = makeGraphNodes(readFile)\n","            \n","            g = mainFunc(all_nodes)\n","\n","            g2 = g \n","            g3 = []\n","            for row in g2:\n","              g3.append(\",\".join(str(item) for item in row))\n","              \n","            g4 = \"[[\"\n","            g4 += \"],[\".join(str(item) for item in g3) + \"]]\"\n","            \n","            writeFile = writeFolder + str(subfileNo) + \".txt\"\n","            with open(writeFile,'w') as f:\n","              f.write(g4)\n","            f.close()\n","'''"],"execution_count":null,"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport csv\\nimport scipy.signal\\nimport numpy as np\\nimport pandas as pd\\nfrom skimage import io\\nimport os  \\n    \\ndef makeFolder(directory,parent_dir):\\n      # os.makedirs() method will raise  \\n      # an OSError if the directory to be created already exists  \\n          \\n      # Path  \\n      path = os.path.join(parent_dir, directory)  \\n          \\n      # Create the directory  \\n      os.makedirs(path)  \\n      print(\"Directory \\'% s\\' created\" % directory) \\n      return 1\\n\\n\\n#for all A01-9 folder\\nfor folder in range(8,10):\\n    #for all A01T_1-4 folder\\n    for file in range(3,5):\\n        s = \"A0\" + str(folder) + \"E_\" + str(file);   #read file name\\n        readFolder = \"Segmentation_after_channel_selection/A0\"+ str(folder)+\"/Testing/\" + s + \"/\"   #read file location\\n        writeFolder = \"Graphs/A0\"+str(folder)+\"/Testing/\"  #save location\\n        #makeFolder(s,writeFolder)\\n        writeFolder += s + \"/\"\\n        dirListing = os.listdir(readFolder)\\n        print(len(dirListing))\\n        file_count = len(dirListing)\\n        for subfileNo in range(1,95):\\n            readFile = readFolder + str(subfileNo) + \".csv\"\\n            print(readFile)\\n            #training = pd.read_csv(readFile)\\n            all_nodes = makeGraphNodes(readFile)\\n            \\n            g = mainFunc(all_nodes)\\n\\n            g2 = g \\n            g3 = []\\n            for row in g2:\\n              g3.append(\",\".join(str(item) for item in row))\\n              \\n            g4 = \"[[\"\\n            g4 += \"],[\".join(str(item) for item in g3) + \"]]\"\\n            \\n            writeFile = writeFolder + str(subfileNo) + \".txt\"\\n            with open(writeFile,\\'w\\') as f:\\n              f.write(g4)\\n            f.close()\\n'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"G24pDIC64H_l","executionInfo":{"elapsed":9,"status":"error","timestamp":1632889060918,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"},"user_tz":-330},"outputId":"df42780c-71e7-476b-8216-b9a94326ead6"},"source":["\n","'''\n","def makeFolder(directory,parent_dir):\n","      path = os.path.join(parent_dir, directory)  \n"," \n","      os.makedirs(path)  \n","      print(\"Directory '% s' created\" % directory)\n","      return 1\n","\n","\n","s = \"A01T_1\";   #read file name\n","readFile = \"Segmentation/A01/Training/\" + s  + \"/4.csv\"  #read file location\n","saveDirectory = \"Graphs/A01/Training/\"  #save location\n","all_nodes = makeGraphNodes(readFile)\n","print(\"################# All Nodes ##############\")\n","print(\"\")\n","print(all_nodes)\n","\n","\n","g = mainFunc(all_nodes)\n","arr = np.array(g)\n","DF = pd.DataFrame(arr)\n","csvFile = saveDirectory + s + \"/4.csv\"\n","DF.to_csv(r\"%s\"%csvFile, index = False, header = False)\n","\n","\n","li2 = [ y for x in g for y in x]\n","','.join(map(str,li2))\n","'''"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eae762150c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mreadFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Segmentation/A01/Training/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m\"/4.csv\"\u001b[0m  \u001b[0;31m#read file location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msaveDirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Graphs/A01/Training/\"\u001b[0m  \u001b[0;31m#save location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mall_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeGraphNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"################# All Nodes ##############\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makeGraphNodes' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"5pDoFzdyd5jW","executionInfo":{"elapsed":8,"status":"error","timestamp":1633783170771,"user":{"displayName":"Kartik Rai 5-Yr IDD Comp. Sci. & Engg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15050916069074999926"},"user_tz":-330},"outputId":"28c2fbc6-d402-4318-f353-1b1d935c5eaa"},"source":["'''\n","filename = saveDirectory + s + \"/trial1.txt\"\n","#g.to_csv(r\"%s\"%loc,header=False,index=False)\n","g2 = g \n","g3 = []\n","for row in g2:\n","  g3.append(\",\".join(str(item) for item in row))\n","\n","g4 = \"[[\"\n","g4 += \"],[\".join(str(item) for item in g3) + \"]]\"\n","\n","print(\"g2::\" , g4)\n","\n","with open(filename,'w') as f:\n","  f.write(g4)\n","\n","\n","  # c3, c4, c8 automatic cchannel selection in EEG signals for classification 2017\n","'''"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-ae078d078c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveDirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/trial1.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#g.to_csv(r\"%s\"%loc,header=False,index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'saveDirectory' is not defined"]}]},{"cell_type":"code","metadata":{"id":"CN3pGKPx8zer"},"source":["#bispectrum features selection\n","'''\n","from turbustat.statistics import Bispectrum\n","from astropy.io import fits\n","import matplotlib.pyplot as plt\n","\n","s = \"A01T_1\";   #read file name\n","readFile = \"Segmentation/A01/Training/\" + s  + \"/1.csv\"  #read file location\n","saveDirectory = \"Graphs/A01/Training/\"  #save location\n","moment0 = pd.read_csv(readFile)\n","\n","# bispec = Bispectrum(moment0)\n","\n","spectrum.bispectrumi(moment0[0], nlag=None, nsamp=None, overlap=None, flag='biased', nfft=None, wind=None)\n","\n","\n","for node_no in range (0,22):\n","    print(\"\\n\", node_no)\n","    labelling = [0] * 20\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMD1pUQwNEHR","executionInfo":{"elapsed":523,"status":"ok","timestamp":1636293397434,"user":{"displayName":"Kartik Rai 5-Yr IDD Comp. Sci. & Engg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15050916069074999926"},"user_tz":-330},"outputId":"ac00b111-d861-4e70-bf6d-73c3c13a2bee"},"source":["#for genrating the respective directories\n","\n","import csv\n","import scipy.signal\n","import numpy as np\n","import pandas as pd\n","from skimage import io\n","import os \n","def makeFolder(directory,parent_dir):\n","      # os.makedirs() method will raise  \n","      # an OSError if the directory to be created already exists  \n","          \n","      # Path  \n","      path = os.path.join(parent_dir, directory)  \n","          \n","      # Create the directory  \n","      os.makedirs(path)  \n","      print(\"Directory '% s' created\" % directory) \n","      return 1\n","\n","for folder in range(2,4):\n","    #for all A01E_1-4 folder\n","    for file in range(1,5):\n","        s = \"A0\" + str(folder) + \"E_\" + str(file);   #read file name\n","        writeFolder = \"GraphsNodes/A0\"+str(folder)+\"/Testing/\"  #save location\n","        makeFolder(s,writeFolder)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Directory 'A02E_1' created\n","Directory 'A02E_2' created\n","Directory 'A02E_3' created\n","Directory 'A02E_4' created\n","Directory 'A03E_1' created\n","Directory 'A03E_2' created\n","Directory 'A03E_3' created\n","Directory 'A03E_4' created\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9HF_lOhpzLT","executionInfo":{"status":"ok","timestamp":1636528711218,"user_tz":-330,"elapsed":2912199,"user":{"displayName":"Yagya Modi 4-Yr B.Tech. Comp. Sci. & Engg., IIT(BHU), Varanasi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2F-OmS0RiNgcIOTYGf6C1mNgAdONV44Di6Y8pg=s64","userId":"13482982178016417491"}},"outputId":"e03de3c1-a140-470e-99b1-210f019827d3"},"source":["import csv\n","import scipy.signal\n","import numpy as np\n","import pandas as pd\n","from skimage import io\n","import os \n","\n","def makeFolder(directory,parent_dir):\n","      # os.makedirs() method will raise  \n","      # an OSError if the directory to be created already exists  \n","          \n","      # Path  \n","      path = os.path.join(parent_dir, directory)  \n","          \n","      # Create the directory  \n","      os.makedirs(path)  \n","      print(\"Directory '% s' created\" % directory) \n","      return 1\n","\n","\n","#for all A01-9 folder\n","#currently running for folder 1 ONLY \n","for folder in range(2,3):\n","    #for all A01T_1-4 folder\n","    for file in range(4,5):\n","        s = \"A0\" + str(folder) + \"E_\" + str(file);   #read file name\n","        readFolder = \"Segmentation_after_channel_selection/A0\"+ str(folder)+\"/Testing/\" + s + \"/\"   #read file location\n","        writeFolder = \"GraphsNodes/A0\"+str(folder)+\"/Testing/\"  #save location\n","        #makeFolder(s,writeFolder)\n","        writeFolder += s + \"/\"\n","        dirListing = os.listdir(readFolder)\n","        print(len(dirListing))\n","        file_count = len(dirListing)\n","        for subfileNo in range(1,file_count+1):\n","            readFile = readFolder + str(subfileNo) + \".csv\"\n","            print(readFile)\n","            #training = pd.read_csv(readFile)\n","            all_nodes = makeGraphNodes(readFile)\n","            \n","            #g = mainFunc(all_nodes)\n","\n","            g2 = all_nodes \n","            g3 = []\n","            for row in g2:\n","              g3.append(\",\".join(str(item) for item in row))\n","              \n","            g4 = \"[[\"\n","            g4 += \"],[\".join(str(item) for item in g3) + \"]]\"\n","            \n","            writeFile = writeFolder + str(subfileNo) + \".txt\"\n","            with open(writeFile,'w') as f:\n","              f.write(g4)\n","            f.close()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["145\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/1.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/2.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/3.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/4.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/5.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/6.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/7.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/8.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/9.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/10.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/11.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/12.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/13.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/14.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/15.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/16.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/17.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/18.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/19.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/20.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/21.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/22.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/23.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/24.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/25.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/26.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/27.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/28.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/29.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/30.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/31.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/32.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/33.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/34.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/35.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/36.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/37.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/38.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/39.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/40.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/41.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/42.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/43.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/44.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/45.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/46.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/47.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/48.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/49.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/50.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/51.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/52.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/53.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/54.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/55.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/56.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/57.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/58.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/59.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/60.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/61.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/62.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/63.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/64.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/65.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/66.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/67.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/68.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/69.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/70.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/71.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/72.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/73.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/74.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/75.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/76.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/77.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/78.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/79.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/80.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/81.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/82.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/83.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/84.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/85.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/86.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/87.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/88.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/89.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/90.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/91.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/92.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/93.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/94.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/95.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/96.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/97.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/98.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/99.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/100.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/101.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/102.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/103.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/104.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/105.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/106.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/107.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/108.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/109.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/110.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/111.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/112.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/113.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/114.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/115.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/116.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/117.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/118.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/119.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/120.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/121.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/122.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/123.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/124.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/125.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/126.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/127.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/128.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/129.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/130.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/131.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/132.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/133.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/134.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/135.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/136.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/137.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/138.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/139.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/140.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/141.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/142.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/143.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/144.csv\n","Segmentation_after_channel_selection/A02/Testing/A02E_4/145.csv\n"]}]}]}